---
layout: single
title: "Building Trust in AI Part III: Trusting the Algorithmic Prediction and Outcome"
date: 2018-04-6 20:03
author: Chukwuka Orefo
comments: true
categories: [AI, Artificial Intelligence, Business, Policy, Technology]
---

![](/images/AI-Rise-of-the-Bots-Slideshow.jpg "robotic frog")

As AI and machine learning algorithms become more complicated and less transparent and an awareness of our dependence on such algorithmic decision-making increases; a call for algorithmic transparency shortly follows.

Trust has to do a lot with transparency it is the idea that we should be able to provide some level of explanations about how an algorithm comes to its decisions. The narrative is very simple; if we don't trust a classifier, we simply won't use it. At this level of abstraction trust is achieved through the ability to think critically and carefully about information or results presented to us. To  accomplish this we must be able to ask questions or  at the very least --understand how or why a value came to be  in order to make what we believe to be best long term decision for the future. As such critical thought is the hallmark of communication and awareness that enables us to understand if information is incomplete, biased or otherwise skewed. Critical thought is logos of democracy. As such a decision via an algorithm doesn’t automatically make it reliable and trustworthy; just like quantifying something with data doesn’t automatically make it true. Despite the difficulty that the world faces with the transparency of AI and machine learning algorithms It’s important to point out that contrary to what most people believe we humans species are not transparent to ourselves or our thought processes. Most of the time we justify our actions and behaviors after the fact or we just don’t think about them at all unless questioned. This is because critical thinking is cognitively expensive task. Instead we are largely ruled by a cognitive committee of synaptic computations which occur mostly in our limbic system. Neuroscientist are still trying to understand synaptic computations of our brains that give rise to consciousness. AI and machine learning algorithms by design make inductions, identify distinctions our through artificially generated synaptic computations loosely based on the biological mechanisms of the brain. As such it is important to for policymakers, journalists, scientist and engineers understand  that a complete and meaningful transparency regarding the mechanisms underlying the  decision making  of machine learning algorithms is akin to the pursuit of unlocking the mechanisms behind consciousness --which is the holy grail of neuroscience. However despite the opaqueness and lack of transparency of our own minds degrees of transparency have allowed cooperation, negotiation and interactions between all humans of all nations exist. These degrees of transparency express themselves in the  form of social cues and language. It is at this level of transparency that has allowed mankind to build trust and corporate enough to build families, societies, nations and even travel to the moon. As such in order to build a level of  trust and cooperation between man and expert machines it is therefore essential that AI and machine learning systems are able to “explain” their decisions  to us in a way that we can understand. Accomplishing this moves is further into the domains of anthropomorphism but it is biological need nonetheless. Understanding the insights of an AI and machine learning model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.

#### _Lost in translation_

Trust between AI and machine learning systems and principle agents (humans) is thought best accomplished through AI natural language explications. However here is were we as humans, potentially have a dramatic shortcoming. All human languages are shaped by our perceptions which is in turn defined the amount of stuff --the number of variables we can consciously process and or pay attention to at any given time; this number is limited. Natural human languages alone are not enough to describe our world so we invented and further developed our mathematics a bespoke but highly low level-language descriptive which sits on top of human natural language. As contradictory as that statement may sound from a software engineering and computer programming perspective it’s true. Mathematics help us better describe, test and theories about certain aspects of the known and unknown universe. The use of  mathematics in general everyday decision making and comprehensible conversation is limited. The primary benefit and function of AI and machine learning systems is derived from the ability of these systems to process seemingly limitless variables with data at a much faster are than humans can; then finding correlations between datasets --leapfrogging our discoveries towards new heights. AI and machine learning systems speed up the process towards new insights. These systems perform black boxed inductive and deductive reasoning about the data then categorize --codify these distinctions into their own computational language and spit out correlations already present within a dataset for us to make use of. As such AI and machine learning systems may learn distinctions for which there could be words. However given the number of variables such systems can process it clear to see that their descriptive vocabulary greatly dwarfs our own. Simply put there just aren't words in any human languages to describe the world as “perceived” by current AI and machine learning systems. The post-linguistic emergent concepts that exist in AI and machine learning systems could over time, some of could be incorporated into human language and humans may come to understand and utilize. For example, the incorporation of new words and concepts have been occurring within the English language since the birth and growth of the internet; the word tweet now has a new meaning in the digital-sphere. Such evolution and adoption of language however does take time; however given the rapid growth and adoption of AI and machine learning systems into society it’s highly unlikely that such post-linguistic emergent concepts will be making their way into the common vocabulary anytime soon. In addition try to distil raw AI and machine learnings post-linguistic distinctions into something we can linguistically comprehend we’d likely run into something of an embodiment problem as embodiment seems to be how we humans iteratively frame our cognition. Pursuing this line of engineering in order to achieve linguistic comprehension of Ai and machine learning systems would likely lead to system bottlenecking which would to some degree negate the very benefits that AI and machine learning systems currently award us.

Perhaps the optimum way to achieve a cooperative relationship transparency between mankind and machine learning systems is through compromise. Instead of trying to get a machine learning system to completely explain its actions we can meet machine learning systems some of the way there by guiding an individual's mental model of exception based reality, towards its reality via expertise combine with  inductive and deductive insight as to the decision taken by the machine learning system. For example the AI or robot has some internal state that determines its actions that the principal agent in question cannot know however, what the individuals can have is some sort of estimate or belief in what the AI or robots outcome will be via physical or Symbolic linguistic representation that will enable person(s) heuristic representation and observations to continually change an re-estimate thus adjusting their mental model over time. The implementation of reward function can be employed to maximize the AI agents towards actions that steer the principal agents belief system towards the correct mental model of be through exaggerated human comprehensible motion or providing indications through social cues so that becomes more clear to a person observing steering the person's belief of reality via input responsiveness. This right now is perhaps the best way to achieve a degree of transparency which instills trust.

A paper published in 2016 by [*Ribeiro, et al;*](https://arxiv.org/abs/1602.04938) from the University of Washington titled Why Should I Trust You? Explaining the Predictions of Any Classifier presented an agnostic technique that explains the predictions of any classifier. Their method presents representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

![Explaining the Predictions of Any Classifier ](/images/why should i trust you2.png "Why Should I Trust You? Explaining the Predictions by of Any Classifier Ribeiro et al 2016")
_Figure.1 Ribeiro et al 2016_

*Ribeiro, et al;* demonstrate the flexibility of their methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). They also show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust.

![Explaining the Predictions of Any Classifier ](/images/why should i trust you1.png "Why Should I Trust You? Explaining the Predictions by of Any Classifier Ribeiro et al 2016")
_Figure.2 Ribeiro et al 2016_

For example, in the imagine above a neural network model process all the information we know about a patient and then tell informs us that the patient in question likely has the flu. In addition the neural network model also guides the doctors mental model of exception based reality, towards its reality using visual symbolic linguistic representation and As such the doctor is able to induce from the information provided by the neural network is that given that the patient has a headache and sneezes contributes towards the conclusion that said patient has the flu, but, however the lack of fatigue is notable evidence against it.

Trying to make AI systems that can explain itself while laudable ideal but it is not a very well thought out one which can quickly be realised when taken to its logical conclusion. An AI system that can understand even a modicum of it’s processed combined with what can be considered a highly rational evidence based thought process with no emotional mechanism and hence no embodied ethic or morals means that it can and will be willing lie to us (game the system) if it deems it adventitious. An AI system that can reason with us, explain its actions and see and describe the world in many more dimensions than humanly possible (perhaps even the future from a deterministic point of view) simple will not share out human values which are rooted in the past and our present needs. To such a machine we will seem like children or even worse just another set of ape within the set of possible apes and how often do you think about the everyday goings of your wildlife ape? Such an AI will make suggestions that will greatly improve our lives and the environment however those improvements will likely conflict with well established traditions, habits and values. Like the spoilt children we are we will demand to be acknowledged and when we are not treated according to our individual values. We will attempt to “fix” the AI to make it comply our very short sighted needs. This is when the war between man and machine will reach it’s apex.

_Hope this helps..._
