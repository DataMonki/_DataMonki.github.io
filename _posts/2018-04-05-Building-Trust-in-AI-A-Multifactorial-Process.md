---
layout: post
title: Building Trust in AI --A Multifactorial Process
date: 2018-04-18 20:03
author: By Chukwuka Orefo
comments: true
categories: [AI, Artificial Intelligence, Business, Technology, Policy, Governance]
---


Today AI and machine learning systems and their algorithms continue to permeate areas of social and economic decision-making, steadily replacing traditional systems and social structures which have traditionally been left to human judgment.

While technologists have long used algorithms to manipulate data --creating software that can analyse information based on rules and logic; AI and machine learning algorithms in comparison don’t make decisions like traditional software. Instead algorithmic software that can --with limited or no human intervention, modify its processing based on data input. We are truly lucky to be alive, to be involved; to bare witness to an amazing time in history. What is exactly that has made humans so great at building technology? Insights from psychology, neuroscience and behavioral economics, tell us that it’s our brains, more specifically it’s our ability to cooperate with one another to, to come together to solve problems and achieve grand  feats greater than the sum of our individual parts. It’s our ability to trust. It’s trust the builds families, societies, nations and allows us to travel to the moon.  trust is dynamic -- that trust is indispensable. Especially when we don't know what to expect. Trust means vulnerability it is the risk we take. --the leap of good faith that we bestow upon a person or entity with hopes that they too have our best intentions in mind. Trust is the catalysts and stabilizing element for the proper flourishing societies, cities, businesses and our economies. However trust is quickly eroded when we realize our its been misplaced or not reciprocated. Once a reputation is tarnished it’s difficult to rebuild. It seems like the element of trust has been somewhat overlooked by the corporations, engineers and researchers who design, testing and deploy AI and machine learning systems. AI and machine learning are more alien than human like, more statistical than algorithmic. Whether humans are directly using AI and machine learning systems as tools, or are deploying models within other products, a vital concern remains: if the users have no insight or control over AI and machine learning decision making process or how it came to a prediction, people will not use it adoption will cease and we will have entered a new AI winter.  

As such establishing trust between humans and machine learning systems is essential for the future. As such being able to --in someway explain individual predictions is important in establishing trust. In today's society we take trust for granted because it works so well. However trust is a multifactorial and dynamic process, as such as it is important to differentiate between factors which of trust that need to be established and fully realized respect to AI and machine learning systems before we can take full advantage of it all it has to offer. I will discuss five main areas of trust I believe are vital for the proper functions

(1) Trust in The Data
(2) Trust in The Algorithmic model
(3) Trust in The AI's Predictions
(4) Trust in Businesses
(5) Trust in Government
(6) Trust from The Public



Trust in the Data
training data and training features are central to any machine learning project.
Your machines are learning, but what’s it being taught and who’s teaching it? that much more in AI because machines don’t bring prior experience, contextual beliefs Machines only learn from only what you show them The textbook in machine learning is the “training data” (some data you’ve examined and labeled with the answer you want) which you show to your software to teach it how to make decisions. Often it's a corpus of historical data used to training machine learning algorithms is riddled with backend in bias Which is drawn from so The teacher decides what questions to ask, and tells learners what matters the teacher is responsible for “feature selection.
Algorithms are becoming more and more involved in major decisions in many industries. They are already helping to decide who gets a loan, who is hired or fired, who can travel freely, and even who is arrested and how long they go to jail. If these algorithms are flawed or biased, it could actually amplify injustice and inequality.  AI and machine learning systems learn by finding patterns and data. the input data is used to train AI and machine learning systems it’s natural to think that because machine learning is an automated there is no human bias. However, just because a process has been automated based on data it doesn't automatically make that process neutral. In fact a “neutral” learning algorithm can yield a model that strongly deviates from the actual population statistics, or from a morally justifiable type of model, simply because the input or training data is biased in some way. AI is only as sharp as the data it learns from. Compounding this effect further is the convenience and reusability of code libraries. Many programmers including myself use popular coding libraries to do common tasks. If the code being reused contains bias, this bias will be reproduced. Bias can get business, a large companies and governments into trouble. AI systems that develop undesired biases can become problematic.

Even with good intentions it's a cognitively difficult task to separate ourselves from our own human biases. This is  because bias is one of the primary organizing principles —one of our brains primary decision-making systems. It helps us build schemas which compound into heuristics that are then used to interpret, navigate, interact with our environment. Without bias we would not be able to prioritize as it tells us what to ignore and what to pay attention to. However bias is a double edged sword as there are unwanted biases which can then persist into the information environment leading to beliefs which develop into stereotypes which then leads to prejudice when acted upon causing inequality, unrest and societal tensions. Our biases, reflect our values and culture. These biases for better or for worse are a part of us; they inform our decision making, our policies and even the technology we create. Bias can manifest in many different ways. For example dataset spanning over ten years of candidates who have been interviewed and joined a company could be used to train a machine learning system to automatically select candidates for future postings. If the original data contains implicit bias such as hiring disproportionately more of one population demographic than another --say more white men than women or minorities --or perhaps paying higher salaries to a particular demographic of the population than another. Such biases, albeit unintentional will be carried forward into the machine learning models of the future. As such candidates that were hired in the past become reinforced. Additionally a study at MIT Media Lab found that leading facial recognition systems correctly identified white male faces 99 percent of the time. But with dark-skinned female faces, it made mistakes up to 35 percent of the time. That’s likely because the data used to train software is often overwhelmingly white and male One widely used data set was estimated to be more than 75 percent male and 80 percent white. ProPublica recently investigated the algorithms used in courtrooms to predict the risk of criminals committing future crimes. These “risk assessments” can be used to help make critical decisions, like who can be released from prison and how much the bail amount should be. They found that the algorithms falsely predicted black defendants to be future criminals at almost twice the rate of white defendants. As such business will need to take a serious look at and be ever vigilant about bias in their datasets because a machine are no human and so cannot tell the difference between an underlying pattern and a bias in the data.

Machine learning algorithms can’t recognise bias and as such will fail to adapt to the changes in socio-economic climate which will may affect the growth and reputation business, nation or government.

When collecting data we need to make sure the dataset is pulled is randomized and accurately reflects the entire population. It is also important allocate time so screen for selection bias Here are some examples of selection bias that engineers researcher and corporations should be aware of:

voluntary bias -when the subjects who volunteer to participate in a research project are different in some ways from the general population which skews the data and thus the results

undercoverage bias - when the subjects or data provided is not representative of the population which is not represented in the results

Non-response bias - when a certain groups of people do not participate and so the data is not represented in the results

convenience bias - when selecting member of a population who are conveniently and readily available  so you just ask people who are convenient yet for you to ask which skews the data and thus the results
Focus Bias- the deliberate use or non-use of certain types of legally required or protected information respectively t--which can lead to biased algorithmic decision relative to a moral standard.

By screening for selection bias present in our datasets then documenting our findings and methods we used to address data bias goes on a long way in the eyes of the public. This helps to establish trust between you and the public which will in turn work towards building trust in the promise of AI.


Trust in the Algorithmic Model
As well as trusting the unbiasedness of the data, there is also need to evaluate the model as a whole before deploying it “in the wild”. Users need to be confident that the model will perform well on real-world data; and according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation datasets. However, real-world data is often significantly different. Furthermore, the evaluation such metrics may not be indicative of the product’s intended goal. Thus the thorough inspection of individual predictions and their explanations before deployment is a worthwhile solution. There are aspects related to the design of machine learning model that can have inbuilt bias. One such bias is the bias function of an estimator.

Estimator bias function
Bias function of an estimator is a measure of central tendency. It is the difference between the estimator expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased. Otherwise the estimator is said to be biased. In statistics, "bias" is an objective property of an estimator, and while not a desired property, it is not pejorative, unlike the ordinary English use of the term bias. Bias can also be measured with respect to the median, rather than the mean expected value.

The average of a set of data points tells us something about the data as a whole, but it doesn’t tell us about individual data points. As such sometimes the mean is misleading. In order to give unusually large or small values, also called outliers, less influence on our measure of where the center of our data is, we can use the median. However the mode is most useful when you have a relatively large sample so that you have a large number. So naturally there might be good reasons to use a statistically biased estimator; most notably, it might be to facilitate a significant reduced variance on small sample sizes in this way the use of a bias function is a deliberate choice. We consciously choose to use a “biased” algorithm in order to mitigate or compensate for other types of biases. For example, if one is concerned about the biasing impacts of training data, then many algorithms provide smoothing or regularization parameters that help to reduce the possibility of over fitting noisy or anomalous input data.

Bias can also arise from the inappropriate uses or deployment of the algorithms and autonomous systems. when the algorithm or resulting model is deployed outside of those contexts, then it will not necessarily perform according to its intended use case or appropriate standards. For example, an owner of a self-driving car in the United States decides to import their new car to the United kingdom. The autonomous systems within the car when driven within the United Kingdom would perform in an unwanted --biased manner since in the united kingdom we drive on the left-hand side of the road and not on the right. This biased performance arises from inappropriate use outside of an intended contexts. A more subtle example of transfer context bias could arise in translating a healthcare algorithm or autonomous system from a research hospital to a rural clinic. Almost certainly, the system would have significant algorithmic bias relative to a statistical standard, as the transfer context likely have quite different characteristics. This statistical bias could also be a moral bias if, say, the autonomous system assumed that the same level of resources were available, and so made morally flawed healthcare resource allocation decisions. Subtle nuances such as these have turned out to be socio political and economic tripwires for big businesses when step on because in many of these cases the fall out could have completely been avoided with an increased investment in the diversity technology field and among the groups of people that are actually creating the algorithms. An increase in the diversity culture brings with it an increase in the diversity of thoughts, considerations, perspectives and values. This is of incredible important in the now increasingly shared hyper-connected world where businesses, lives and livelihoods and reputations can be dismantled overnight with a Youtube, Facebook or twitter message. Because without diversity biases can be baked into these algorithms, and so they'll behave in a prejudiced way, just like a biased person would. An investment in diversity is only really 50% of the journey the rest boils down to documentation and standard operating procedures. Documenting the use cases and hence it’s limitations as well as justifying the statistical methods used in the design of our machine learning models and making these white papers publically available for appraisal will take us that much closer to build trust in the promise of AI.  



Building Trust: Data Prediction and Outcome
As AI and machine learning algorithms become more complicated and less transparent and an awareness of our dependence on such algorithmic decision-making increases; a call for algorithmic transparency shortly follows. Trust has to do a lot with transparency it is the idea that we should be able to provide some level of explanations about how an algorithm comes to its decisions. The narrative is very simple; if we don't trust a classifier, we simply won't use it. At this level of abstraction trust is achieved through the ability to think critically and carefully about information or results presented to us. To  accomplish this we must be able to ask questions or  at the very least --understand how or why a value came to be  in order to make what we believe to be best long term decision for the future. As such critical thought is the hallmark of communication and awareness that enables us to understand if information is incomplete, biased or otherwise skewed. Critical thought is logos of democracy. As such a decision via an algorithm doesn’t automatically make it reliable and trustworthy; just like quantifying something with data doesn’t automatically make it true. Despite the difficulty that that the world faces with the transparency of AI and machine learning algorithms It’s important to point out that contrary to what most people believe we humans species are not transparent to ourselves or our thought processes. Most of the time we justify our actions and behaviors after the fact or we just don’t think about them at all unless questioned. This is because critical thinking is cognitively expensive task. Instead we are largely ruled by a cognitive committee of synaptic computations which occur mostly in our limbic system. Neuroscientist are still trying to understand synaptic computations of our brains that give rise to consciousness. AI and machine learning algorithms by design make inductions, identify distinctions our through artificially generated synaptic computations loosely based on the biological mechanisms of the brain. As such it is important to for policymakers, journalists, scientist and engineers understand  that a complete and meaningful transparency regarding the mechanisms underlying the  decision making  of machine learning algorithms is akin to the pursuit of unlocking the mechanisms behind consciousness --which is the holy grail of neuroscience. However despite the opaqueness and lack of transparency of our own minds degrees of transparency have allowed cooperation, negotiation and interactions between all humans of all nations exist. These degrees of transparency express themselves in the  form of social cues and language. It is at this level of transparency that has allowed mankind to build trust and corporate enough to build families, societies, nations and even travel to the moon. As such in order to build a level of  trust and cooperation between man and expert machines it is therefore essential that AI and machine learning systems are able to “explain” their decisions  to us in a way that we can understand. Accomplishing this moves is further into the domains of anthropomorphism but it is biological need nonetheless. Understanding the insights of an AI and machine learning model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.


Lost in translation
Trust between AI and machine learning systems and principle agents (humans) is thought best accomplished through AI natural language explications. However here is were we as humans, potentially have a dramatic shortcoming. All human languages are shaped by our perceptions which is in turn defined the amount of stuff --the number of variables we can consciously process and or pay attention to at any given time; this number is limited. Natural human languages alone are not enough to describe our world so we invented and further developed our mathematics a bespoke but highly low level-language descriptive which sits on top of human natural language. As contradictory as that statement may sound from a software engineering and computer programming perspective it’s true. Mathematics help us better describe, test and theories about certain aspects of the known and unknown universe. The use of  mathematics in general everyday decision making and comprehensible conversation is limited. The primary benefit and function of AI and machine learning systems is derived from the ability of these systems to process seemingly limitless variables with data at a much faster are than humans can; then finding correlations between datasets --leapfrogging our discoveries towards new heights. AI and machine learning systems speed up the process towards new insights. These systems perform black boxed inductive and deductive reasoning about the data then categorize --codify these distinctions into their own computational language and spit out correlations already present within a dataset for us to make use of. As such AI and machine learning systems may learn distinctions for which there could be words. However given the number of variables such systems can process it clear to see that their descriptive vocabulary greatly dwarfs our own. Simply put there just aren't words in any human languages to describe the world as “perceived” by current AI and machine learning systems. The post-linguistic emergent concepts that exist in AI and machine learning systems could over time, some of could be incorporated into human language and humans may come to understand and utilize. For example, the incorporation of new words and concepts have been occurring within the English language since the birth and growth of the internet; the word tweet now has a new meaning in the digital-sphere. Such evolution and adoption of language however does take time; however given the rapid growth and adoption of AI and machine learning systems into society it’s highly unlikely that such post-linguistic emergent concepts will be making their way into the common vocabulary anytime soon. In addition try to distil raw AI and machine learnings post-linguistic distinctions into something we can linguistically comprehend we’d likely run into something of an embodiment problem as embodiment seems to be how we humans iteratively frame our cognition. Pursuing this line of engineering in order to achieve linguistic comprehension of Ai and machine learning systems would likely lead to system bottlenecking which would to some degree negate the very benefits that AI and machine learning systems currently award us.

Perhaps the optimum way to achieve a cooperative relationship transparency between mankind and machine learning systems is through compromise. Instead of trying to get a machine learning system to completely explain its actions we can meet machine learning systems some of the way there by guiding an individual's mental model of exception based reality, towards its reality via expertise combine with  inductive and deductive insight as to the decision taken by the machine learning system. For example the AI or robot has some internal state that determines its actions that the principal agent in question cannot know however, what the individuals can have is some sort of estimate or belief in what the AI or robots outcome will be via physical or Symbolic linguistic representation that will enable person(s) heuristic representation and observations to continually change an re-estimate thus adjusting their mental model over time. The implementation of reward function can be employed to maximize the AI agents towards actions that steer the principal agents belief system towards the correct mental model of be through exaggerated human comprehensible motion or providing indications through social cues so that becomes more clear to a person observing steering the person's belief of reality via input responsiveness. This right now is perhaps the best way to achieve a degree of transparency which instills trust.

A paper Ribeiro et al  from the University of Washington published in 2016 titled Why Should I Trust You? Explaining the Predictions of Any Classifier presented an agnostic technique that explains the predictions of any classifier. Their method presents representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. They demonstrate the flexibility of their methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). They also show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust.


Figure (1) Ribeiro et al 2016
For example, in the imagine above a neural network model process all the information we know about a patient and then tell informs us that the patient in question likely has the flu. In addition the neural network model also guides the doctors mental model of exception based reality, towards its reality using visual symbolic linguistic representation and As such the doctor is able to induce from the information provided by the neural network is that given that the patient has a headache and sneezes contributes towards the conclusion that said patient has the flu, but, however the lack of fatigue is notable evidence against it.


Figure (2) Ribeiro et al 2016

Trust in Business

Data Begets Data
In today's steadily growing digital economy information is the new currency, data is the new oil and attention is a scarce resource. The amount of data on earth is doubling at a rate that is arguably unmeasurable right now. There are many of studies about data and they are all looking at different things. --How much data is transferred on the Internet? --How much data is stored on devices? All of these questions are proxies; the truth is no one really knows anymore. The thing is that data begets data so just having more information doesn't mean we necessarily have more grounded truth. In an information-rich world the abundant wealth of information means a dearth of something else --the scarcity of whatever it is that information consumes which in our case is the attention of us its intended recipients. Companies that build our favorite digital tools don't just collect data they monetize. Companies are now competing with each other to see who can get the most data on you and then offer it to advertisers we are not just their customers we are what they sell. However it isn’t all bad as it means that it’s easy for companies to create different web pages for different people. Sometimes that customization is helpful, such as when you see search results for restaurants near you. Moreover consumers of today have become almost expecting that they're going to get personalization from the private sector. However sometimes it can be creepy, such as when ads follow you around from website to website. Sometimes customization can cost you money, for instance Orbitz showed higher-priced hotels to owners of Mac computers, for instance.


Emotional contagion
Facebook knows who all your online friends are. It has seen all your photos; it uses facial recognition on all your devices, it knows where you live yet Facebook they still buys data from data brokers about your offline life to enhance their profile on you like what car you drive the cost of your mortgage and what you buy at the supermarket even though this data is often sloppy or inaccurate it's a multi-billion dollar industry well I do this to serve you targeted ads of course they call this the optimization of your Facebook experience but maybe more accurately it's the monetization of your behavior.

In 2014 Facebook released a paper were their researchers to manipulate the News Feeds of 689,003 of their users showing that emotional states can be transferred to others via emotional contagion. Emotional contagion is this idea that emotions, both positive and negative, can be transferred between people. The 2014 study showed that emotional contagion can occur outside of in-person interaction between individuals by reducing the amount of emotional content in the News Feed. Simply but when they reduce the amount of positive news to participants feed, these individuals reported more negativity in general (less positive expressions). The research observed the opposite effect (more positive expression) when they instead reduce the amount of negative news to participants new feed. What they showed was that it is actually possible to lead a group of people to experience the same emotions without their awareness and this can be without an direct interaction between people, and in the complete absence of nonverbal cues . So it turns out Facebook and other social media platforms at large can deliberately alter your emotions and hence how you think. This gives the market among other things an easier more strategic doorway into manipulating your mind. For instance studies show that women feel most vulnerable on Mondays and feel best about themselves on Thursdays so naturally particular ads are delivered during peak vulnerability moments. In addition, let's say a woman just posted a selfie with the hashtag beautiful... well it follows then that to best target her the market must align with her current emotions and feed her with weekend style, fashion and weekend fun the a slogan such as how to be a “true independent woman.”

The Belmont Report was written by the National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research. The Commission, created as a result of the National Research Act of 1974,  summarizes ethical principles and guidelines for research that should underlie the conduct of biomedical and behavioral research involving human subjects Three core principles are identified where informed consent, assessment of risks and benefits, and selection of subjects. The Declaration of Helsinki is a set of ethical principles regarding human experimentation developed for the medical community by the World Medical Association. Although it is not a legally binding instrument under the international law it is widely regarded as the cornerstone document on human research ethics. The fundamental principle of the Declaration of Helsinki boil down to  respect for the individual (Article 8), their right to self-determination and the right to make informed decisions (Articles 20, 21 and 22) regarding participation in research, both initially and during the course of the research. The investigator's duty is solely to the patient (Articles 2, 3 and 10) or volunteer (Articles 16, 18), and while there is always a need for research (Article 6), the subject's welfare must always take precedence over the interests of science and society (Article 5), and ethical considerations must always take precedence over laws and regulations (Article 9).

An editor on the study, Susan Fiske, told The Guardian that she had worries about Facebook's study. Saying that:

["People are supposed to be told they are going to be participants in research and then agree to it and have the option not to agree to it without penalty."]--Susan Fiske 2014

Susan Fiske’s comment suggests that despite Facebook knowledge of best practices research and ethics and international guidelines Facebook still did not feel the need to inform their users of the nature of their research or the potential impacts of it. Can this and other cases like this such as the fake news debacle be taken as evidence that best practices research, ethics and international guideline in AI and machine learning systems which are not enforced by international law will largely be ignored by big digital corporations?  

9-1 Negativity Bias
when we look at high performance in the brain there's one part that's really important and it's called the prefrontal cortex (PFC) and this is the part of the brain that makes us different from animals it's the part of the brain that has come very late in evolution and it's a part of the brain we need for rational processing --higher-level processing and decision making and that's what companies are looking for in the people they hire --those who can really perform well and have well-functioning brains this is also the part of the brain that matures only at age 18 or 21. The PFC is the part of the brain that's also responsible for inhibition and delay of gratification and executive control which is the ability to delay gratification and plan ahead and to not always get all things at once. As such the PFC is implicated in addiction. The PFC is always overruled by another part of the brain and that's called the limbic system. This is because there limbic system there's myriad of connections projecting to the PFC than vise versa and so the limbic system has a much greater influence on the PFC. The limbic system is the part of our brain where pleasure, reward, emotion, memory and reinforcement are processed it's a part of the brain that's way older from an evolutionary perspective. When people are in a reward state dopamine is released from the  limbic system and this will have a very positive impact on the functioning of the PFC so people who are in a good mood and are feeling well will perform better. when people are in a threat mode your PFC shuts off triggering the fight or flight response important to make quick decisions you don't want to think of ten different solutions of escaping a tiger you just run okay and social situations kick off our threat circuit but they are not really a danger to our life but we experience them our brain processes them just as if somebody is about to hurt you and as such you won't have access to your full mental power anymore. 9-1 negativity bias; between reward and threat is the brain does not treat positive stimulus and negative equally we process negative experiences nine times more strongly than positive experiences negative information is like velcro. We are now experiencing real time manipulation by our dopamine and by companies that have understood how to manipulate our attitudes through gamification of our dopamine responses. It‘s become common place for companies use AI and machine learning algorithms to maximize dopamine response; they know how to make a system that maximizes user engagement to make them money. It is surely no coincidence, then, that the activities social networking sites foster fake news, gossip, virtue signaling, rumor-mongering and the ever-shifting movements of popular culture and fad.

The AI and machine learning algorithms which accomplish this task are largely hidden from view, remaining opaque even when we are prompted to examine them. They are rarely subject to the same checks and balances as human decision-makers.  Google  uses algorithms for almost all of its services; everything from search engine results to driving directions on Google Maps --it can make connections that aren't just strange but actually discriminatory. For instance consider the case of the Google search results leading up to the 2012 presidential election in which people who typed Obama were shown Obama results in their subsequent searches but those who queried Romney were not shown any Romney results in subsequent searches. Google response to this was that their machine learning algorithm had found that people who searched for Obama wanted more Obama results and those who searched for Romney didn't. For a traditional media company such disparity would be viewed as providing biased coverage however it's more difficult to blame a machine for bias in the era of algorithms. The question that arises is in an ever growing age of digital customization how do we strike a balance between the side effects of personalization?

Corporations are using AI and machine learning algorithms to consciously exploit a vulnerabilities in human psychology. Not only have AI and machine learning algorithms become  interrupting but it perhaps even feel normal for a lot of us to naturally keep checking and reaching our phone with the expectation that there’s a notification. Perhaps you know what I'm talking about, perhaps you do this yourself, and perhaps your doing this right now while reading this post… Thing is we don’t do this consciously, but habitually just like Pavlov's dog which is famous example of classical conditioning. So let’s put this into context, if Pavlov’s Bell is your notification ringtone and the treat is that one message you a day you might get that makes you happy.

AI becomes the tool of inattention a potential tool for the dismantling of democracy. We are often losing sight of the actual trade-off because we are so focused on the benefit. We really need to think very hard about what makes corporations behave unethically it's not the people within them. There is something about our current business models and economic systems that makes corporations behave legally and unethically and that means either we need to change the laws to make more behaviors illegal or we need to rethink the incentives of the system.
